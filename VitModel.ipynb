{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyYhDtV6fYbC",
        "outputId": "564bcc3b-7282-417c-c9b7-87c79105d3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TewGin3YgL_V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4dssNw5gST-",
        "outputId": "97b04243-b07e-4f7d-d920-11decc47576d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n",
            "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
          ]
        }
      ],
      "source": [
        "num_classes = 5\n",
        "input_shape = (32,32,3)\n",
        "\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\"/content/gdrive/MyDrive/flower_photos\")\n",
        "train = tf.keras.preprocessing.image_dataset_from_directory(\"/content/gdrive/MyDrive/flower_photos\",\n",
        "                                                            validation_split=0.2,\n",
        "                                                            labels='inferred',\n",
        "                                                            subset=\"training\",\n",
        "                                                            image_size= (72,72),\n",
        "                                                            batch_size = 200,\n",
        "                                                            seed=123)\n",
        "validation = tf.keras.preprocessing.image_dataset_from_directory(\"/content/gdrive/MyDrive/flower_photos\",\n",
        "                                                                 validation_split = 0.2,\n",
        "                                                                 labels=\"inferred\",\n",
        "                                                                 subset=\"validation\",\n",
        "                                                                 batch_size = 200,\n",
        "                                                                 image_size = (72,72),\n",
        "                                                                 seed=123)\n",
        "class_names = dataset.class_names\n",
        "print(class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89ad07r82BiN"
      },
      "outputs": [],
      "source": [
        "'''train_iterator = train.as_numpy_iterator()\n",
        "train_ds = train_iterator.next()\n",
        "print(train_ds[0].shape)\n",
        "validation_iterator = validation.as_numpy_iterator()\n",
        "validation_ds = validation_iterator.next()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvMyg1hKB4YH"
      },
      "outputs": [],
      "source": [
        "image_size = 72\n",
        "preprocessingModel = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "augmentedModel = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\"),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomRotation(factor = 0.02),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomZoom(\n",
        "              height_factor = 0.2,width_factor = 0.2\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MJcXdnSB8jg"
      },
      "outputs": [],
      "source": [
        "train = train.map(lambda x,y:(preprocessingModel(x),y))\n",
        "validation = validation.map(lambda x,y:(preprocessingModel(x),y))\n",
        "train = train.map(lambda x,y:(augmentedModel(x),y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqnE3dP0EGl1"
      },
      "outputs": [],
      "source": [
        "train = train.prefetch(tf.data.AUTOTUNE)\n",
        "validation = validation.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmhQ_tiLkJie"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self , size , num_of_patches , projection_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.size=size\n",
        "    self.num_of_patches= num_of_patches + 1\n",
        "    self.projection_dim=projection_dim\n",
        "\n",
        "    self.projection=tf.keras.layers.Dense(projection_dim)\n",
        "\n",
        "    self.clsToken = self.add_weight(\n",
        "            name=\"clsToken\",\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"HeNormal\",  # Experiment with different initializers\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , projection_dim)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    patches = tf.image.extract_patches(inputs , sizes=[1 , self.size , self.size , 1], strides=[1 , self.size , self.size , 1], rates=[1 ,1 ,1 ,1], padding=\"VALID\",)\n",
        "\n",
        "    patches=tf.reshape(patches, (tf.shape(inputs)[0], -1, self.size * self.size *3))\n",
        "    patches= self.projection(patches)\n",
        "\n",
        "    # repeat cls token length of batch size\n",
        "    clsToken = tf.repeat(self.clsToken , tf.shape(inputs)[0] , 0)\n",
        "    patches = tf.concat((clsToken, patches) , axis=1)\n",
        "    # create position number for each patch\n",
        "    positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]\n",
        "    positionalEmbedding = self.positionalEmbedding(positions)\n",
        "\n",
        "    #print(positionalEmbedding)\n",
        "    patches= patches + positionalEmbedding\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWMMTdVVkNeY"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, heads, mlp_rate, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(heads, d_model//heads, dropout=dropout_rate)\n",
        "\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(d_model * mlp_rate, activation=\"gelu\"),  # Changed \"Relu\" to \"relu\"\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        out_1 = self.layernorm_1(inputs)\n",
        "        out_1 = self.mha(out_1, out_1, training=training)\n",
        "        out_1 = inputs + out_1\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1)\n",
        "        out_2 = self.mlp(out_2, training=training)\n",
        "        out_2 = out_1 + out_2\n",
        "\n",
        "        out_3 = self.layernorm_3(out_2)\n",
        "        return out_3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSoKTk5RkQt8"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self , d_model , heads , mlp_rate , num_layers=1 , dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoders = [TransformerLayer(d_model , heads , mlp_rate , dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "  def call(self , inputs , training=True):\n",
        "    x =inputs\n",
        "\n",
        "    for layer in self.encoders:\n",
        "      x = layer(x , training=training)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh77JGYBkSLq"
      },
      "outputs": [],
      "source": [
        "class ViT(tf.keras.Model):\n",
        "  def __init__(self , num_classes , patch_size , num_of_patches , d_model , heads , num_layers , mlp_rate , dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patchEmbedding = PatchEmbedding(patch_size , num_of_patches , d_model)\n",
        "    self.encoder = TransformerEncoder(d_model , heads , mlp_rate  ,num_layers , dropout_rate)\n",
        "    self.encoderNormalization = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.prediction = tf.keras.Sequential([\n",
        "                                           tf.keras.layers.Dropout(0.2),\n",
        "                                           tf.keras.layers.Dense(mlp_rate * d_model , activation=\"gelu\"),\n",
        "                                           tf.keras.layers.Dropout(0.2),\n",
        "                                           tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "\n",
        "  ])\n",
        "  def call(self , inputs ,  training=True):\n",
        "    patches = self.patchEmbedding(inputs) #patches will contain patch + positional information\n",
        "    encoderResult = self.encoder(patches, training=training)\n",
        "\n",
        "    clsResult = encoderResult[: , 0 , :]\n",
        "    clsResult = self.encoderNormalization(clsResult)\n",
        "    prediction = self.prediction(clsResult,\n",
        "                                 training=training)\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFsZ1aL5lIz4"
      },
      "outputs": [],
      "source": [
        "def convert_to_dataset(data,batch_size,shuffle = False,augment = False):\n",
        "  dataset1 = data.map(lambda x,y:(preprocessingModel(x)[0],y),num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset1 = dataset1.shuffle(len(dataset1))\n",
        "\n",
        "  dataset1 = dataset1.batch(batch_size,drop_remainder = True)\n",
        "  if augment:\n",
        "    dataset1 = dataset1.map(lambda x,y:(augmentedModel(x,training = True),y),num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset1.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M0UAf3blMl1"
      },
      "outputs": [],
      "source": [
        "vitClassifier = ViT(\n",
        "                5,\n",
        "                6,\n",
        "                (72//6)**2,\n",
        "                64,\n",
        "                5,\n",
        "                4,\n",
        "                3,\n",
        "                0.1\n",
        ")\n",
        "\n",
        "vitClassifier.compile(\n",
        "  optimizer=\"adam\",\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=[\n",
        "      tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.SparseTopKCategoricalAccuracy(10, name=\"top-10-accuracy\"),\n",
        "  ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQMJLaAwlOtT"
      },
      "outputs": [],
      "source": [
        "history = vitClassifier.fit(train,batch_size=200,validation_data=validation,epochs=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}